{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02b72e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_with_ngrams</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cubs infielder Matt Shaw defends missing game ...</td>\n",
       "      <td>2025-09-24T16:54:50.717Z</td>\n",
       "      <td>https://www.nbcnews.com/news/us-news/cubs-matt...</td>\n",
       "      <td>Minyvonne Burke</td>\n",
       "      <td>Chicago Cubs infielder Matt Shaw said he thoug...</td>\n",
       "      <td>['chicago', 'cubs', 'infielder', 'matt', 'shaw...</td>\n",
       "      <td>['chicago', 'cubs', 'infielder', 'matt', 'shaw...</td>\n",
       "      <td>chicago cubs infielder matt shaw think importa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YouTube to start bringing back creators banned...</td>\n",
       "      <td>2025-09-24T16:54:08.647Z</td>\n",
       "      <td>https://www.nbcnews.com/tech/tech-news/youtube...</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>YouTube will offer creators a way to rejoin th...</td>\n",
       "      <td>['youtube', 'offer', 'creator', 'way', 'rejoin...</td>\n",
       "      <td>['youtube', 'offer', 'creator', 'way', 'rejoin...</td>\n",
       "      <td>youtube offer creator way rejoin stream platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A trio of space weather satellites blast off t...</td>\n",
       "      <td>2025-09-24T14:15:16.424Z</td>\n",
       "      <td>https://www.nbcnews.com/science/science-news/t...</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>CAPE CANAVERAL, Fla. â€” A cluster of space weat...</td>\n",
       "      <td>['cape', 'canaveral', 'fla.', 'cluster', 'spac...</td>\n",
       "      <td>['cape', 'canaveral', 'fla.', 'cluster', 'spac...</td>\n",
       "      <td>cape canaveral fla. cluster space weather sate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump administration rehires hundreds of feder...</td>\n",
       "      <td>2025-09-24T13:35:34.846Z</td>\n",
       "      <td>https://www.nbcnews.com/politics/trump-adminis...</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>MIAMI â€” Hundreds of federal employees who lost...</td>\n",
       "      <td>['miami', 'federal', 'employee', 'lose', 'job'...</td>\n",
       "      <td>['miami', 'federal', 'employee', 'lose', 'job'...</td>\n",
       "      <td>miami federal employee lose job elon_musk cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NASA introduces its newest astronauts</td>\n",
       "      <td>2025-09-23T13:15:55.028Z</td>\n",
       "      <td>https://www.nbcnews.com/science/science-news/n...</td>\n",
       "      <td>The Associated Press</td>\n",
       "      <td>CAPE CANAVERAL, Fla. â€” NASA introduced its new...</td>\n",
       "      <td>['cape', 'canaveral', 'fla.', 'nasa', 'introdu...</td>\n",
       "      <td>['cape', 'canaveral', 'fla.', 'nasa', 'introdu...</td>\n",
       "      <td>cape canaveral fla. nasa introduce new astrona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Cubs infielder Matt Shaw defends missing game ...   \n",
       "1  YouTube to start bringing back creators banned...   \n",
       "2  A trio of space weather satellites blast off t...   \n",
       "3  Trump administration rehires hundreds of feder...   \n",
       "4              NASA introduces its newest astronauts   \n",
       "\n",
       "                       date  \\\n",
       "0  2025-09-24T16:54:50.717Z   \n",
       "1  2025-09-24T16:54:08.647Z   \n",
       "2  2025-09-24T14:15:16.424Z   \n",
       "3  2025-09-24T13:35:34.846Z   \n",
       "4  2025-09-23T13:15:55.028Z   \n",
       "\n",
       "                                                link                author  \\\n",
       "0  https://www.nbcnews.com/news/us-news/cubs-matt...       Minyvonne Burke   \n",
       "1  https://www.nbcnews.com/tech/tech-news/youtube...  The Associated Press   \n",
       "2  https://www.nbcnews.com/science/science-news/t...  The Associated Press   \n",
       "3  https://www.nbcnews.com/politics/trump-adminis...  The Associated Press   \n",
       "4  https://www.nbcnews.com/science/science-news/n...  The Associated Press   \n",
       "\n",
       "                                             content  \\\n",
       "0  Chicago Cubs infielder Matt Shaw said he thoug...   \n",
       "1  YouTube will offer creators a way to rejoin th...   \n",
       "2  CAPE CANAVERAL, Fla. â€” A cluster of space weat...   \n",
       "3  MIAMI â€” Hundreds of federal employees who lost...   \n",
       "4  CAPE CANAVERAL, Fla. â€” NASA introduced its new...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['chicago', 'cubs', 'infielder', 'matt', 'shaw...   \n",
       "1  ['youtube', 'offer', 'creator', 'way', 'rejoin...   \n",
       "2  ['cape', 'canaveral', 'fla.', 'cluster', 'spac...   \n",
       "3  ['miami', 'federal', 'employee', 'lose', 'job'...   \n",
       "4  ['cape', 'canaveral', 'fla.', 'nasa', 'introdu...   \n",
       "\n",
       "                                  tokens_with_ngrams  \\\n",
       "0  ['chicago', 'cubs', 'infielder', 'matt', 'shaw...   \n",
       "1  ['youtube', 'offer', 'creator', 'way', 'rejoin...   \n",
       "2  ['cape', 'canaveral', 'fla.', 'cluster', 'spac...   \n",
       "3  ['miami', 'federal', 'employee', 'lose', 'job'...   \n",
       "4  ['cape', 'canaveral', 'fla.', 'nasa', 'introdu...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  chicago cubs infielder matt shaw think importa...  \n",
       "1  youtube offer creator way rejoin stream platfo...  \n",
       "2  cape canaveral fla. cluster space weather sate...  \n",
       "3  miami federal employee lose job elon_musk cost...  \n",
       "4  cape canaveral fla. nasa introduce new astrona...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"nbc_articles_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ed645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 2. DEFINE SPLIT RANGES\n",
    "# ====================================================\n",
    "TRAIN_START = pd.to_datetime(\"2011-12-01\").date()\n",
    "TRAIN_END   = pd.to_datetime(\"2023-12-31\").date()\n",
    "VAL_START   = pd.to_datetime(\"2024-01-01\").date()\n",
    "VAL_END     = pd.to_datetime(\"2024-04-14\").date()\n",
    "TEST_START  = pd.to_datetime(\"2024-04-15\").date()\n",
    "TEST_END    = pd.to_datetime(\"2025-04-14\").date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 6. ASSIGN SPLIT LABEL\n",
    "# ====================================================\n",
    "def get_split(d):\n",
    "    if TRAIN_START <= d <= TRAIN_END:\n",
    "        return \"train\"\n",
    "    elif VAL_START <= d <= VAL_END:\n",
    "        return \"val\"\n",
    "    elif TEST_START <= d <= TEST_END:\n",
    "        return \"test\"\n",
    "    else:\n",
    "        return \"exclude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 1. LOAD SPACY MODEL (English)\n",
    "# ====================================================\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])  # POS only, faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cfb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 5. POS-TAGGING: KEEP ONLY NOUN, PRONOUN, ADJECTIVE, VERB\n",
    "# ====================================================\n",
    "def filter_pos(text):\n",
    "    doc = nlp(str(text))\n",
    "    allowed_pos = {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\", \"PRON\"}\n",
    "    filtered = [t.text for t in doc if t.pos_ in allowed_pos]\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfcea2c",
   "metadata": {},
   "source": [
    "1. Split Tesla articles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7246a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Grouped by date â€” total unique days: 3198\n",
      "ðŸ” Running POS tagging and filtering... (this may take a few minutes)\n",
      "âœ… Saved as 'nbc_articles_split.csv'\n",
      "split\n",
      "train    2033\n",
      "test      310\n",
      "val        71\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 3. LOAD DATA\n",
    "# ====================================================\n",
    "df = pd.read_csv(\"nbc_articles_cleaned.csv\")\n",
    "\n",
    "# ensure date column is datetime.date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "\n",
    "# ====================================================\n",
    "# 4. GROUP BY DATE: CONCATENATE CLEAN_TEXT\n",
    "# ====================================================\n",
    "# Combine all article texts in one day\n",
    "grouped_df = (\n",
    "    df.groupby(\"date\", as_index=False)\n",
    "      .agg({\n",
    "          \"clean_text\": lambda x: \" \".join(x.astype(str))  # merge texts\n",
    "      })\n",
    ")\n",
    "\n",
    "print(f\"âœ… Grouped by date â€” total unique days: {len(grouped_df)}\")\n",
    "print(\"ðŸ” Running POS tagging and filtering... (this may take a few minutes)\")\n",
    "grouped_df[\"clean_text\"] = grouped_df[\"clean_text\"].apply(filter_pos)\n",
    "\n",
    "grouped_df[\"split\"] = grouped_df[\"date\"].apply(get_split)\n",
    "\n",
    "# ====================================================\n",
    "# 7. KEEP ONLY RELEVANT COLUMNS\n",
    "# ====================================================\n",
    "df_final = grouped_df[[\"date\", \"clean_text\", \"split\"]]\n",
    "df_final = df_final[df_final[\"split\"] != \"exclude\"]\n",
    "\n",
    "# ====================================================\n",
    "# 8. SAVE TO NEW FILE\n",
    "# ====================================================\n",
    "df_final.to_csv(\"nbc_articles_split.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved as 'nbc_articles_split.csv'\")\n",
    "print(df_final[\"split\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f131bb",
   "metadata": {},
   "source": [
    "2. Split market articles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde09fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Grouped by date â€” total unique days: 6740\n",
      "ðŸ” Running POS tagging and filtering... (this may take a few minutes)\n",
      "âœ… Saved as 'market_articles_split.csv'\n",
      "split\n",
      "train    3639\n",
      "test      363\n",
      "val       100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 3. LOAD DATA\n",
    "# ====================================================\n",
    "df = pd.read_csv(\"market_articles_cleaned.csv\")\n",
    "\n",
    "# ensure date column is datetime.date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "\n",
    "# ====================================================\n",
    "# 4. GROUP BY DATE: CONCATENATE CLEAN_TEXT\n",
    "# ====================================================\n",
    "# Combine all article texts in one day\n",
    "grouped_df = (\n",
    "    df.groupby(\"date\", as_index=False)\n",
    "      .agg({\n",
    "          \"clean_text\": lambda x: \" \".join(x.astype(str))  # merge texts\n",
    "      })\n",
    ")\n",
    "\n",
    "print(f\"âœ… Grouped by date â€” total unique days: {len(grouped_df)}\")\n",
    "print(\"ðŸ” Running POS tagging and filtering... (this may take a few minutes)\")\n",
    "grouped_df[\"clean_text\"] = grouped_df[\"clean_text\"].apply(filter_pos)\n",
    "\n",
    "grouped_df[\"split\"] = grouped_df[\"date\"].apply(get_split)\n",
    "\n",
    "# ====================================================\n",
    "# 7. KEEP ONLY RELEVANT COLUMNS\n",
    "# ====================================================\n",
    "df_final = grouped_df[[\"date\", \"clean_text\", \"split\"]]\n",
    "df_final = df_final[df_final[\"split\"] != \"exclude\"]\n",
    "\n",
    "# ====================================================\n",
    "# 8. SAVE TO NEW FILE\n",
    "# ====================================================\n",
    "df_final.to_csv(\"market_articles_split.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved as 'market_articles_split.csv'\")\n",
    "print(df_final[\"split\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f75ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
