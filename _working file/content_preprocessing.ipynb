{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956707d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e89bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.symbols import PROPN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97acb4a1",
   "metadata": {},
   "source": [
    "#### **1. Tokenize, lemmatization, remove stopwords** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbe446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nlp():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 4_000_000\n",
    "\n",
    "    # 1) Mark protected terms before NER\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = [\n",
    "        {\"label\": \"PROTECTED_PROPN\", \"pattern\": [{\"LOWER\": \"elon\"}, {\"LOWER\": \"musk\"}]},\n",
    "        {\"label\": \"PROTECTED_PROPN\", \"pattern\": [{\"LOWER\": \"tesla\"}]},\n",
    "        {\"label\": \"PROTECTED_PROPN\", \"pattern\": [{\"LOWER\": \"spacex\"}]},\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    # 2) Register + add custom component\n",
    "    @Language.component(\"merge_and_force_propn\")\n",
    "    def merge_and_force_propn(doc):\n",
    "        spans = [ent for ent in doc.ents if ent.label_ == \"PROTECTED_PROPN\"]\n",
    "        with doc.retokenize() as retok:\n",
    "            for sp in spans:\n",
    "                retok.merge(\n",
    "                    sp,\n",
    "                    attrs={\"pos\": PROPN, \"tag\": \"NNP\", \"lemma\": sp.text.lower(), \"ent_type\": sp.label}\n",
    "                )\n",
    "        for tok in doc:\n",
    "            if tok.ent_type_ == \"PROTECTED_PROPN\":\n",
    "                tok.pos = PROPN\n",
    "                tok.tag_ = \"NNP\"\n",
    "        return doc\n",
    "\n",
    "    if \"merge_and_force_propn\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"merge_and_force_propn\", after=\"ner\")\n",
    "\n",
    "    return nlp\n",
    "\n",
    "nlp = build_nlp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0585079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 351\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2) Stopwords (expand aggressively)\n",
    "# -----------------------------------------\n",
    "BASE_STOPS = set(nlp.Defaults.stop_words)\n",
    "MORE_STOPS = {\n",
    "    # generic filler / vague words\n",
    "    \"people\",\"think\",\"use\",\"thing\",\"want\",\"time\",\"work\",\"life\",\"world\",\"way\",\"good\",\"new\",\"year\",\n",
    "    \"day\",\"come\",\"look\",\"talk\",\"right\",\"like\",\"make\",\"find\",\"need\",\"get\",\"say\",\"take\",\"know\", \"we\", \n",
    "    \"i\", \"that\", \"lot\", \"m\"\n",
    "    # twitter/web artifacts\n",
    "    \"amp\",\"rt\"\n",
    "}\n",
    "STOPWORDS = BASE_STOPS | MORE_STOPS\n",
    "# keep protected terms\n",
    "for keep in [\"elon\", \"musk\", \"elon musk\", \"tesla\", \"spacex\"]:\n",
    "    for part in keep.split():\n",
    "        STOPWORDS.discard(part)\n",
    "    STOPWORDS.discard(keep)\n",
    "\n",
    "print(f\"Number of stopwords: {len(STOPWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elon_musk', 'tesla', 'car', 'amazing', 'visit', 'https://tesla.com']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# 3) Clean -> tokenize -> lemmatize -> stopwords\n",
    "# ------------------------------------------------\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "\n",
    "def to_tokens(text: str):\n",
    "    \"\"\"\n",
    "    Returns a list of final tokens:\n",
    "      - punctuation & spaces removed\n",
    "      - protected terms merged and kept as single tokens: elon_musk, tesla, spacex\n",
    "      - lowercased\n",
    "      - lemmatized (except we keep lemma for words; for PROPN we use lower form)\n",
    "      - stopwords removed\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = normalize_whitespace(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        # Skip spaces/punctuation\n",
    "        if tok.is_space or tok.is_punct:\n",
    "            continue\n",
    "\n",
    "        # Build raw lower form\n",
    "        lower = tok.text.lower()\n",
    "\n",
    "        # Collapse protected multi-word into underscored token\n",
    "        # (after merging, protected is one token with ent_type_ set)\n",
    "        if tok.ent_type_ == \"PROTECTED_PROPN\":\n",
    "            if lower == \"elon musk\":\n",
    "                norm = \"elon_musk\"\n",
    "            else:\n",
    "                norm = lower  # \"tesla\" or \"spacex\"\n",
    "            tokens.append(norm)\n",
    "            continue\n",
    "\n",
    "        # Lemma (use lemma_ for normal words; for propn keep surface lower)\n",
    "        if tok.pos_ == \"PROPN\":\n",
    "            norm = lower\n",
    "        else:\n",
    "            lemma = tok.lemma_.lower()\n",
    "            # Some lemmas are \"-PRON-\" or \"\" in small models; fallback to lower\n",
    "            norm = lemma if lemma and lemma != \"-pron-\" else lower\n",
    "\n",
    "        # Remove tokens that are stopwords or purely numeric or leftover punctuation\n",
    "        if norm in STOPWORDS:\n",
    "            continue\n",
    "        if norm.isnumeric():\n",
    "            continue\n",
    "        if not re.search(r\"[a-z0-9_]\", norm):\n",
    "            continue\n",
    "\n",
    "        tokens.append(norm)\n",
    "\n",
    "    return tokens\n",
    "# Example usage\n",
    "text = \"  Elon Musk's Tesla cars are amazing! Visit https://tesla.com  \"\n",
    "print(to_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbb33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elon_musk', 'tesla', 'car', 'amazing', 'visit', 'https://tesla.com', 'elon_musk_tesla', 'tesla_car', 'car_amazing', 'amazing_visit', 'visit_https://tesla.com', 'elon_musk_tesla_car', 'tesla_car_amazing', 'car_amazing_visit', 'amazing_visit_https://tesla.com', 'visit_https://tesla.com_elon_musk_tesla', 'https://tesla.com_elon_musk_tesla_tesla_car', 'elon_musk_tesla_tesla_car_car_amazing', 'tesla_car_car_amazing_amazing_visit', 'car_amazing_amazing_visit_visit_https://tesla.com']\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# 4) N-gram helpers\n",
    "# ------------------\n",
    "def make_ngrams(tokens, n=2):\n",
    "    return [\"_\".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def token_pipeline(text: str, add_bigrams=True, add_trigrams=True):\n",
    "    toks = to_tokens(text)\n",
    "    if add_bigrams:\n",
    "        toks += make_ngrams(toks, 2)\n",
    "    if add_trigrams:\n",
    "        toks += make_ngrams(toks, 3)\n",
    "    return toks\n",
    "    \n",
    "# Example usage\n",
    "text = \"  Elon Musk's Tesla cars are like shit. Don't buy it anymore! Visit https://tesla.com  \"\n",
    "print(token_pipeline(text, add_bigrams=True, add_trigrams=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfc45827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 5) Apply to your DataFrame (example usage)\n",
    "# ------------------------------------------\n",
    "df = pd.read_csv(\"nbc_articles_with_content_official.csv\")\n",
    "df[\"tokens\"] = df[\"content\"].apply(lambda t: to_tokens(t))\n",
    "df[\"tokens_with_ngrams\"] = df[\"content\"].apply(lambda t: token_pipeline(t))\n",
    "df[\"clean_text\"] = df[\"tokens\"].apply(lambda ts: \" \".join(ts))\n",
    "df.to_csv(\"nbc_articles_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec451b",
   "metadata": {},
   "source": [
    "#### **2. Topic modelling - Article level** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2e748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article-level LDA using only PROPN+NOUN.\n"
     ]
    }
   ],
   "source": [
    "# 1) POS-filtered tokens: keep only PROPN & NOUN (Tesla, SpaceX, Autopilot, battery, etc.)\n",
    "def to_tokens_nouns(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for tok in doc:\n",
    "        if tok.is_space or tok.is_punct or tok.like_url or tok.like_email:\n",
    "            continue\n",
    "        if tok.ent_type_ == \"PROTECTED_PROPN\":\n",
    "            out.append(tok.lemma_.lower().replace(\" \", \"_\"))\n",
    "            continue\n",
    "        if tok.pos_ not in {\"PROPN\", \"NOUN\"}:\n",
    "            continue\n",
    "        if tok.pos_ == \"PROPN\":\n",
    "            norm = tok.text.lower()\n",
    "        else:\n",
    "            lemma = tok.lemma_.lower()\n",
    "            norm = lemma if lemma and lemma != \"-pron-\" else tok.text.lower()\n",
    "        if norm in STOPWORDS or norm.isnumeric() or not re.search(r\"[a-z0-9_]\", norm):\n",
    "            continue\n",
    "        out.append(norm)\n",
    "    return out\n",
    "\n",
    "# 2) Build article-level text using only nouns/proper nouns\n",
    "df = pd.read_csv(\"nbc_articles_cleaned.csv\")\n",
    "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)  # use original content, not previously-cleaned string\n",
    "df[\"tokens_nouns\"] = df[\"content\"].apply(to_tokens_nouns)\n",
    "df[\"clean_text_nouns\"] = df[\"tokens_nouns\"].apply(lambda ts: \" \".join(ts))\n",
    "df = df[df[\"clean_text_nouns\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "# 3) Article-level LDA using only PROPN+NOUN\n",
    "extra_stops = [\"s\",\"t\",\"u\",\"re\",\"rt\",\"amp\",\"know\",\"don\"]\n",
    "vect = CountVectorizer(\n",
    "    stop_words=extra_stops,\n",
    "    token_pattern=r\"(?u)\\b[a-z0-9_]+\\b\",\n",
    "    lowercase=False,\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")\n",
    "X = vect.fit_transform(df[\"clean_text_nouns\"])\n",
    "\n",
    "n_topics = 10\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, max_iter=20, learning_method=\"batch\",\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "doc_topic = lda.fit_transform(X)\n",
    "topic_term = lda.components_\n",
    "terms = vect.get_feature_names_out()\n",
    "\n",
    "def top_terms_for_topic(k, topn=10):\n",
    "    idx = topic_term[k].argsort()[::-1][:topn]\n",
    "    return \", \".join(terms[i] for i in idx)\n",
    "\n",
    "topic_summary = pd.DataFrame({\n",
    "    \"topic\": range(n_topics),\n",
    "    \"top_terms\": [top_terms_for_topic(k) for k in range(n_topics)]\n",
    "})\n",
    "\n",
    "article_topic_df = pd.DataFrame(doc_topic, columns=[f\"topic_{i}\" for i in range(n_topics)])\n",
    "for col in [\"date\",\"title\",\"link\"]:\n",
    "    if col in df.columns: article_topic_df.insert(0, col, df[col].values)\n",
    "article_topic_df[\"dominant_topic\"] = article_topic_df.filter(like=\"topic_\").idxmax(axis=1)\n",
    "article_topic_df[\"dominant_score\"] = article_topic_df.filter(like=\"topic_\").max(axis=1)\n",
    "\n",
    "topic_summary.to_csv(\"articles_topics_top_terms.csv\", index=False)\n",
    "article_topic_df.to_csv(\"article_topic_distribution.csv\", index=False)\n",
    "print(\"Saved article-level LDA using only PROPN+NOUN.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aada96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
