{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956707d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e89bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.symbols import PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbe446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def build_nlp():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 4_000_000\n",
    "\n",
    "    # 1) Mark protected terms before NER\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = [\n",
    "        {\"label\": \"PROTECTED_PROPN\", \"pattern\": [{\"LOWER\": \"elon\"}, {\"LOWER\": \"musk\"}]},\n",
    "        {\"label\": \"PROTECTED_PROPN\", \"pattern\": [{\"LOWER\": \"tesla\"}]},\n",
    "        {\"label\": \"PROTECTED_PROPN\", \"pattern\": [{\"LOWER\": \"spacex\"}]},\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    # 2) Register + add custom component\n",
    "    @Language.component(\"merge_and_force_propn\")\n",
    "    def merge_and_force_propn(doc):\n",
    "        spans = [ent for ent in doc.ents if ent.label_ == \"PROTECTED_PROPN\"]\n",
    "        with doc.retokenize() as retok:\n",
    "            for sp in spans:\n",
    "                retok.merge(\n",
    "                    sp,\n",
    "                    attrs={\"pos\": PROPN, \"tag\": \"NNP\", \"lemma\": sp.text.lower(), \"ent_type\": sp.label}\n",
    "                )\n",
    "        for tok in doc:\n",
    "            if tok.ent_type_ == \"PROTECTED_PROPN\":\n",
    "                tok.pos = PROPN\n",
    "                tok.tag_ = \"NNP\"\n",
    "        return doc\n",
    "\n",
    "    if \"merge_and_force_propn\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"merge_and_force_propn\", after=\"ner\")\n",
    "\n",
    "    return nlp\n",
    "\n",
    "nlp = build_nlp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0585079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 326\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 2) Stopwords (keep Elon Musk / Tesla etc.)\n",
    "# -----------------------------------------\n",
    "STOPWORDS = {w for w in nlp.Defaults.stop_words}\n",
    "# Ensure protected terms are NOT stopwords\n",
    "for keep in [\"elon\", \"musk\", \"elon musk\", \"tesla\", \"spacex\"]:\n",
    "    STOPWORDS.discard(keep)\n",
    "    # Also discard split forms just in case\n",
    "    for part in keep.split():\n",
    "        STOPWORDS.discard(part)\n",
    "print(f\"Number of stopwords: {len(STOPWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elon_musk', 'tesla', 'car', 'amazing', 'visit', 'https://tesla.com']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# 3) Clean -> tokenize -> lemmatize -> stopwords\n",
    "# ------------------------------------------------\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (text or \"\")).strip()\n",
    "\n",
    "def to_tokens(text: str):\n",
    "    \"\"\"\n",
    "    Returns a list of final tokens:\n",
    "      - punctuation & spaces removed\n",
    "      - protected terms merged and kept as single tokens: elon_musk, tesla, spacex\n",
    "      - lowercased\n",
    "      - lemmatized (except we keep lemma for words; for PROPN we use lower form)\n",
    "      - stopwords removed\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = normalize_whitespace(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        # Skip spaces/punctuation\n",
    "        if tok.is_space or tok.is_punct:\n",
    "            continue\n",
    "\n",
    "        # Build raw lower form\n",
    "        lower = tok.text.lower()\n",
    "\n",
    "        # Collapse protected multi-word into underscored token\n",
    "        # (after merging, protected is one token with ent_type_ set)\n",
    "        if tok.ent_type_ == \"PROTECTED_PROPN\":\n",
    "            if lower == \"elon musk\":\n",
    "                norm = \"elon_musk\"\n",
    "            else:\n",
    "                norm = lower  # \"tesla\" or \"spacex\"\n",
    "            tokens.append(norm)\n",
    "            continue\n",
    "\n",
    "        # Lemma (use lemma_ for normal words; for propn keep surface lower)\n",
    "        if tok.pos_ == \"PROPN\":\n",
    "            norm = lower\n",
    "        else:\n",
    "            lemma = tok.lemma_.lower()\n",
    "            # Some lemmas are \"-PRON-\" or \"\" in small models; fallback to lower\n",
    "            norm = lemma if lemma and lemma != \"-pron-\" else lower\n",
    "\n",
    "        # Remove tokens that are stopwords or purely numeric or leftover punctuation\n",
    "        if norm in STOPWORDS:\n",
    "            continue\n",
    "        if norm.isnumeric():\n",
    "            continue\n",
    "        if not re.search(r\"[a-z0-9_]\", norm):\n",
    "            continue\n",
    "\n",
    "        tokens.append(norm)\n",
    "\n",
    "    return tokens\n",
    "# Example usage\n",
    "text = \"  Elon Musk's Tesla cars are amazing! Visit https://tesla.com  \"\n",
    "print(to_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbb33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elon_musk', 'tesla', 'car', 'amazing', 'visit', 'https://tesla.com', 'elon_musk_tesla', 'tesla_car', 'car_amazing', 'amazing_visit', 'visit_https://tesla.com', 'elon_musk_tesla_car', 'tesla_car_amazing', 'car_amazing_visit', 'amazing_visit_https://tesla.com', 'visit_https://tesla.com_elon_musk_tesla', 'https://tesla.com_elon_musk_tesla_tesla_car', 'elon_musk_tesla_tesla_car_car_amazing', 'tesla_car_car_amazing_amazing_visit', 'car_amazing_amazing_visit_visit_https://tesla.com']\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# 4) N-gram helpers\n",
    "# ------------------\n",
    "def make_ngrams(tokens, n=2):\n",
    "    return [\"_\".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def token_pipeline(text: str, add_bigrams=True, add_trigrams=True):\n",
    "    toks = to_tokens(text)\n",
    "    if add_bigrams:\n",
    "        toks += make_ngrams(toks, 2)\n",
    "    if add_trigrams:\n",
    "        toks += make_ngrams(toks, 3)\n",
    "    return toks\n",
    "    \n",
    "# Example usage\n",
    "text = \"  Elon Musk's Tesla cars are like shit. Don't buy it anymore! Visit https://tesla.com  \"\n",
    "print(token_pipeline(text, add_bigrams=True, add_trigrams=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfc45827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 5) Apply to your DataFrame (example usage)\n",
    "# ------------------------------------------\n",
    "df = pd.read_csv(\"nbc_articles_with_content_official.csv\")\n",
    "df[\"tokens\"] = df[\"content\"].apply(lambda t: to_tokens(t))\n",
    "df[\"tokens_with_ngrams\"] = df[\"content\"].apply(lambda t: token_pipeline(t))\n",
    "df[\"clean_text\"] = df[\"tokens\"].apply(lambda ts: \" \".join(ts))\n",
    "df.to_csv(\"nbc_articles_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ede1e8",
   "metadata": {},
   "source": [
    "#### Concate on the same date + topic modelling ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "684456ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/dauvudangkhoi/Document /Text Analytics/crawl/lib/python3.13/site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dauvudangkhoi/Document /Text Analytics/crawl/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached scipy-1.16.2-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59595734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique days: 3186\n",
      "Saved:\n",
      " - topics_top_terms.csv  (top terms per topic)\n",
      " - daily_topic_distribution.csv  (per-day topic mixture + dominant topic)\n"
     ]
    }
   ],
   "source": [
    "# 3e + 3f: Concat-by-date → Topic Modelling (LDA)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# ---------- 3e. Concat all same-date docs ----------\n",
    "df = pd.read_csv(\"nbc_articles_cleaned.csv\")\n",
    "\n",
    "# ensure date is YYYY-MM-DD\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "df = df.dropna(subset=[\"date\", \"clean_text\"])\n",
    "\n",
    "# concat all cleaned texts per date\n",
    "daily = (df.groupby(\"date\")[\"clean_text\"]\n",
    "           .apply(lambda s: \" \".join(map(str, s.dropna())))\n",
    "           .reset_index(name=\"doc\"))\n",
    "# drop empty docs (if any)\n",
    "daily = daily[daily[\"doc\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "print(f\"Unique days: {len(daily)}\")\n",
    "\n",
    "# ---------- Vectorize ----------\n",
    "# keep underscores in your merged entities/ngrams: elon_musk, tesla_stock, etc.\n",
    "vect = CountVectorizer(\n",
    "    token_pattern=r\"(?u)\\b[a-z0-9_]+\\b\",  # allow underscores\n",
    "    lowercase=False,                      # your text is already normalized\n",
    "    min_df=2,                             # drop ultra-rare tokens (tune)\n",
    "    max_df=0.95                           # drop very common tokens (tune)\n",
    ")\n",
    "X = vect.fit_transform(daily[\"doc\"])\n",
    "\n",
    "# ---------- 3f. Topic Modelling (LDA) ----------\n",
    "# tune n_components (topics) to 8–20; start with 12\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=12,\n",
    "    max_iter=20,\n",
    "    learning_method=\"batch\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "doc_topic = lda.fit_transform(X)     # shape: [n_days, n_topics]\n",
    "topic_term = lda.components_         # shape: [n_topics, n_terms]\n",
    "terms = pd.Index(vect.get_feature_names_out())\n",
    "\n",
    "# ---------- Inspect & Save ----------\n",
    "def top_terms_for_topic(k, topn=15):\n",
    "    idx = topic_term[k].argsort()[::-1][:topn]\n",
    "    return list(zip(terms[idx], topic_term[k, idx]))\n",
    "\n",
    "# table of top terms per topic\n",
    "topn = 15\n",
    "rows = []\n",
    "for k in range(lda.n_components):\n",
    "    tops = top_terms_for_topic(k, topn)\n",
    "    rows.append({\n",
    "        \"topic\": k,\n",
    "        \"top_terms\": \", \".join([w for w, _ in tops])\n",
    "    })\n",
    "topic_summary = pd.DataFrame(rows)\n",
    "\n",
    "# per-day topic distribution + dominant topic\n",
    "daily_topic_df = pd.DataFrame(doc_topic, columns=[f\"topic_{i}\" for i in range(lda.n_components)])\n",
    "daily_topic_df.insert(0, \"date\", daily[\"date\"].values)\n",
    "daily_topic_df[\"dominant_topic\"] = daily_topic_df.filter(like=\"topic_\").idxmax(axis=1)\n",
    "daily_topic_df[\"dominant_score\"] = daily_topic_df.filter(like=\"topic_\").max(axis=1)\n",
    "\n",
    "# save artifacts\n",
    "topic_summary.to_csv(\"topics_top_terms.csv\", index=False)\n",
    "daily_topic_df.to_csv(\"daily_topic_distribution.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - topics_top_terms.csv  (top terms per topic)\")\n",
    "print(\" - daily_topic_distribution.csv  (per-day topic mixture + dominant topic)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b5414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
