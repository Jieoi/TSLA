{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01bb253-96c5-402d-ba28-b0a6f2760f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning mode: POS-filtered nouns\n",
      "Kept rows after cleaning: 19404\n",
      "TF-IDF shape: (19404, 2078)\n",
      "Topic 00: tesla, company, autopilot, tesla team, tesla model, software, owner, tesla owner, tesla autopilot, tesla car, supercharger, vehicle\n",
      "Topic 01: medium, legacy, legacy medium, propaganda, lie, machine, platform, medium propaganda, truth, news, medium lie, link\n",
      "Topic 02: launch, rocket, falcon, starship, flight, engine, starlink, space, spacex, dragon, earth, mar\n",
      "Topic 03: car, model, production, company, tesla car, month, week, cost, part, car company, software, model car\n",
      "Topic 04: twitter, government, world, account, money, company, speech, country, law, platform, america, party\n",
      "Topic 05: day, point, hour, post, night, person, news, house, end, end day, system, 100\n",
      "Topic 06: way, future, civilization, company, kid, money, platform, game, earth, level, need, tunnel\n",
      "Topic 07: team, lot, work, spacex, week, tesla team, spacex team, congrats, game, engineering, congratulation, lot work\n",
      "Saved:\n",
      "- elon_musk_clean_topics.csv  (text, clean, topic_id, topic_score)\n",
      "- nmf_topics_terms.csv        (top terms per topic)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def ensure_nltk():\n",
    "    # tagger (new name first, then fallback)\n",
    "    try:\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "        except Exception:\n",
    "            nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "    # tokenizers / corpora\n",
    "    for pkg in [\"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\"]:\n",
    "        try:\n",
    "            nltk.data.find(f\"corpora/{pkg}\")\n",
    "        except LookupError:\n",
    "            nltk.download(pkg, quiet=True)\n",
    "\n",
    "ensure_nltk()\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "ALL_PATH = Path(\"all_musk_posts.csv\")\n",
    "\n",
    "df_raw = pd.read_csv(ALL_PATH, low_memory=False)\n",
    "text_col = \"fullText\" if \"fullText\" in df_raw.columns else (\"text\" if \"text\" in df_raw.columns else None)\n",
    "if not text_col:\n",
    "    raise ValueError(\"Expected a text column named 'fullText' or 'text'.\")\n",
    "df = df_raw[[text_col, \"createdAt\"]].rename(columns={text_col: \"text\"}).dropna(subset=[\"text\"]).copy()\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "URL_RE   = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "USER_RE  = re.compile(r\"@\\w+\")\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\"  # broad emoji ranges\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"\n",
    "    \"\\U0001F300-\\U0001F5FF\"\n",
    "    \"\\U0001F600-\\U0001F64F\"\n",
    "    \"\\U0001F680-\\U0001F6FF\"\n",
    "    \"\\U0001F700-\\U0001F77F\"\n",
    "    \"\\U0001F780-\\U0001F7FF\"\n",
    "    \"\\U0001F800-\\U0001F8FF\"\n",
    "    \"\\U0001F900-\\U0001F9FF\"\n",
    "    \"\\U0001FA00-\\U0001FA6F\"\n",
    "    \"\\U0001FA70-\\U0001FAFF\"\n",
    "    \"\\U00002702-\\U000027B0\"\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "PUNCT_RE = re.compile(r\"[^\\w\\s$]\")  # keep $ (cashtags: $TSLA)\n",
    "\n",
    "base_stops = set(stopwords.words(\"english\"))\n",
    "extra = {\n",
    "    # conversational fillers\n",
    "    \"rt\",\"amp\",\"im\",\"ive\",\"youre\",\"weve\",\"hes\",\"shes\",\"its\",\"dont\",\"cant\",\"wont\",\n",
    "    \"yeah\",\"ok\",\"okay\",\"true\",\"haha\",\"ha\",\"wow\",\"cool\",\"good\",\"great\",\"thanks\",\"thank\",\n",
    "    \"exactly\",\"right\",\"love\",\"like\",\"really\",\"much\",\"many\",\"one\",\"thing\",\"things\",\n",
    "    \"today\",\"tomorrow\",\"yesterday\",\"time\",\"year\",\"years\",\"people\",\"guys\"\n",
    "}\n",
    "stops = base_stops | extra\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def basic_cleanup(t: str) -> str:\n",
    "    t = t.lower()\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = USER_RE.sub(\" \", t)\n",
    "    t = EMOJI_RE.sub(\" \", t)\n",
    "    t = PUNCT_RE.sub(\" \", t)    \n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "#POS taggger\n",
    "def clean_with_pos(text: str) -> str:\n",
    "    t = basic_cleanup(text)\n",
    "    toks = word_tokenize(t)\n",
    "    if not toks:\n",
    "        return \"\"\n",
    "    tags = pos_tag(toks)\n",
    "    out = []\n",
    "    for w, p in tags:\n",
    "        if w in stops or len(w) <= 2:\n",
    "            continue\n",
    "        if w.startswith(\"$\") or re.fullmatch(r\"\\d+(m|b|%)?\", w):\n",
    "            out.append(w.upper()); continue\n",
    "        if p.startswith(\"NN\"):\n",
    "            out.append(lemm.lemmatize(w))\n",
    "    return \" \".join(out)\n",
    "\n",
    "#Without POS\n",
    "def clean_simple(text: str) -> str:\n",
    "    t = basic_cleanup(text)\n",
    "    out = []\n",
    "    for w in t.split():\n",
    "        if w in stops or len(w) <= 2:\n",
    "            continue\n",
    "        if w.startswith(\"$\") or re.fullmatch(r\"\\d+(m|b|%)?\", w):\n",
    "            out.append(w.upper()); continue\n",
    "        out.append(lemm.lemmatize(w))\n",
    "    return \" \".join(out)\n",
    "\n",
    "try:\n",
    "    _ = pos_tag([\"test\"])\n",
    "    use_pos = True\n",
    "except Exception:\n",
    "    use_pos = False\n",
    "\n",
    "clean_fn = clean_with_pos if use_pos else clean_simple\n",
    "print(f\"Cleaning mode: {'POS-filtered nouns' if use_pos else 'simple (no POS)'}\")\n",
    "\n",
    "df[\"clean\"] = df[\"text\"].apply(clean_fn)\n",
    "\n",
    "df = df[df[\"clean\"].str.split().str.len() >= 3].copy()\n",
    "print(\"Kept rows after cleaning:\", len(df))\n",
    "\n",
    "df[[\"text\", \"clean\"]].to_csv(\"elon_musk_clean_tweets.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "#tfidf\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,3),   \n",
    "    min_df=10,          \n",
    "    max_df=0.85,       \n",
    ")\n",
    "X = tfidf.fit_transform(df[\"clean\"])\n",
    "vocab = np.array(tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "\n",
    "# NMF topic\n",
    "n_topics = 8   \n",
    "nmf = NMF(n_components=n_topics, random_state=42, init=\"nndsvd\", max_iter=400)\n",
    "W = nmf.fit_transform(X)   \n",
    "H = nmf.components_        \n",
    "\n",
    "def topics_table(model, feat_names, topn=12) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for k, comp in enumerate(model.components_):\n",
    "        idx = comp.argsort()[::-1][:topn]\n",
    "        rows.extend([{\"topic\": k, \"term\": feat_names[i], \"weight\": comp[i]} for i in idx])\n",
    "    out = pd.DataFrame(rows)\n",
    "    for k in range(model.n_components):\n",
    "        terms = \", \".join(out[out.topic==k].sort_values(\"weight\", ascending=False)[\"term\"].head(12))\n",
    "        print(f\"Topic {k:02d}: {terms}\")\n",
    "    return out\n",
    "\n",
    "topics_df = topics_table(nmf, vocab, topn=12)\n",
    "\n",
    "df[\"topic_id\"] = W.argmax(axis=1)\n",
    "df[\"topic_score\"] = W.max(axis=1)\n",
    "\n",
    "df.to_csv(\"elon_musk_clean_topics.csv\", index=False, encoding=\"utf-8\")\n",
    "topics_df.to_csv(\"nmf_topics_terms.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"- elon_musk_clean_topics.csv  (text, clean, topic_id, topic_score)\")\n",
    "print(\"- nmf_topics_terms.csv        (top terms per topic)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8355aa-8603-4e90-992f-c962ee9b9e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>clean</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @einarvollset: I read @paulg’s  “How to Mak...</td>\n",
       "      <td>2023-05-07 10:36:27+00:00</td>\n",
       "      <td>wealth hacker painter mid twenty piece</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @BillyM2k: dude bookmarks are an awesome tw...</td>\n",
       "      <td>2023-02-09 20:03:00+00:00</td>\n",
       "      <td>dude bookmark twitter feature twitter</td>\n",
       "      <td>4</td>\n",
       "      <td>0.158176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Event Horizon Balance Beam</td>\n",
       "      <td>2023-05-12 05:52:26+00:00</td>\n",
       "      <td>event horizon balance beam</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @SpaceX: Watch Falcon 9 launch 54 Starlink ...</td>\n",
       "      <td>2022-12-28 09:43:36+00:00</td>\n",
       "      <td>watch falcon launch starlink satellite</td>\n",
       "      <td>2</td>\n",
       "      <td>0.107221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>To get Blue Verified for $7/month, sign up via...</td>\n",
       "      <td>2023-03-23 21:52:23+00:00</td>\n",
       "      <td>month sign web browser</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55053</th>\n",
       "      <td>Sterilizing minor children before the age of c...</td>\n",
       "      <td>2024-12-04 14:08:44+00:00</td>\n",
       "      <td>child age consent surgery</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55074</th>\n",
       "      <td>Extreme birth rate collapse is the biggest dan...</td>\n",
       "      <td>2024-12-03 17:14:13+00:00</td>\n",
       "      <td>birth rate collapse danger civilization</td>\n",
       "      <td>6</td>\n",
       "      <td>0.008933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55081</th>\n",
       "      <td>@ajtourville @Tesla Rep. Khanna is a sensible ...</td>\n",
       "      <td>2024-12-03 15:04:37+00:00</td>\n",
       "      <td>rep khanna moderate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55090</th>\n",
       "      <td>The scale of spending on illegal immigration b...</td>\n",
       "      <td>2024-12-03 02:10:20+00:00</td>\n",
       "      <td>scale spending immigration mind</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55093</th>\n",
       "      <td>Shareholders should control company votes, not...</td>\n",
       "      <td>2024-12-03 01:10:16+00:00</td>\n",
       "      <td>shareholder company judge</td>\n",
       "      <td>4</td>\n",
       "      <td>0.025258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19404 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      RT @einarvollset: I read @paulg’s  “How to Mak...   \n",
       "2      RT @BillyM2k: dude bookmarks are an awesome tw...   \n",
       "3                             Event Horizon Balance Beam   \n",
       "5      RT @SpaceX: Watch Falcon 9 launch 54 Starlink ...   \n",
       "10     To get Blue Verified for $7/month, sign up via...   \n",
       "...                                                  ...   \n",
       "55053  Sterilizing minor children before the age of c...   \n",
       "55074  Extreme birth rate collapse is the biggest dan...   \n",
       "55081  @ajtourville @Tesla Rep. Khanna is a sensible ...   \n",
       "55090  The scale of spending on illegal immigration b...   \n",
       "55093  Shareholders should control company votes, not...   \n",
       "\n",
       "                       createdAt                                    clean  \\\n",
       "0      2023-05-07 10:36:27+00:00   wealth hacker painter mid twenty piece   \n",
       "2      2023-02-09 20:03:00+00:00    dude bookmark twitter feature twitter   \n",
       "3      2023-05-12 05:52:26+00:00               event horizon balance beam   \n",
       "5      2022-12-28 09:43:36+00:00   watch falcon launch starlink satellite   \n",
       "10     2023-03-23 21:52:23+00:00                   month sign web browser   \n",
       "...                          ...                                      ...   \n",
       "55053  2024-12-04 14:08:44+00:00                child age consent surgery   \n",
       "55074  2024-12-03 17:14:13+00:00  birth rate collapse danger civilization   \n",
       "55081  2024-12-03 15:04:37+00:00                      rep khanna moderate   \n",
       "55090  2024-12-03 02:10:20+00:00          scale spending immigration mind   \n",
       "55093  2024-12-03 01:10:16+00:00                shareholder company judge   \n",
       "\n",
       "       topic_id  topic_score  \n",
       "0             4     0.002072  \n",
       "2             4     0.158176  \n",
       "3             4     0.002617  \n",
       "5             2     0.107221  \n",
       "10            3     0.007937  \n",
       "...         ...          ...  \n",
       "55053         4     0.003809  \n",
       "55074         6     0.008933  \n",
       "55081         0     0.000000  \n",
       "55090         4     0.018651  \n",
       "55093         4     0.025258  \n",
       "\n",
       "[19404 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b1c77-d02f-40d1-acae-5e7f3cf093a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
